{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Phi():\n",
    "    def __init__(self):\n",
    "        self.vectorizer = DictVectorizer()\n",
    "    \n",
    "    def fit(self, training_sentences):\n",
    "        counts = []\n",
    "        for sentence in training_sentences:\n",
    "            words = [pair[0] for pair in sentence]\n",
    "            tags = [pair[1] for pair in sentence]\n",
    "            count = self.extract_freatures(words, tags)\n",
    "            counts.append(count)\n",
    "        \n",
    "        #Fit the dictvectorizer to the data\n",
    "        #Fit expects a list of dictonaries but we just give it the one we constructed\n",
    "        self.vectorizer.fit(counts)\n",
    "    \n",
    "    def transform(self, word_sequence, tag_sequence):\n",
    "        #Extract the feature count\n",
    "        count = self.extract_freatures(word_sequence, tag_sequence)\n",
    "        \n",
    "        #Convert the count to a sparse vector using dictVectorizer\n",
    "        vector = self.vectorizer.transform(count)\n",
    "    \n",
    "        return vector\n",
    "    \n",
    "    def extract_freatures(self, word_sequence, tag_sequence):\n",
    "        #Force them to be the same length\n",
    "        length = len(word_sequence)\n",
    "    \n",
    "        #Append end to tags\n",
    "        tag_sequence.append('END')\n",
    "    \n",
    "        #Create a list of word-tag and tag-tag features\n",
    "        features = []\n",
    "        for i in range(0,length):\n",
    "            features.append((word_sequence[i],tag_sequence[i]))\n",
    "            features.append((tag_sequence[i],tag_sequence[i+1]))\n",
    "        \n",
    "        #Create a count out of the list of features\n",
    "        count = Counter(features)\n",
    "        \n",
    "        return count\n",
    "    \n",
    "    def inverse_transform(self, vector):\n",
    "        #Convert the vector to a count using dictVectorizer\n",
    "        counts = self.vectorizer.inverse_transform(vector)\n",
    "        \n",
    "        #inverse_transform returns a list of counts but we only gave it one vector\n",
    "        return counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.brown.tagged_sents(categories='news',tagset='universal')\n",
    "training = corpus[0:3000]\n",
    "testing = corpus[3001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'A', 'A'), ('A', 'A', 'B'), ('A', 'A', 'C'), ('A', 'B', 'A'), ('A', 'B', 'B'), ('A', 'B', 'C'), ('A', 'C', 'A'), ('A', 'C', 'B'), ('A', 'C', 'C'), ('B', 'A', 'A'), ('B', 'A', 'B'), ('B', 'A', 'C'), ('B', 'B', 'A'), ('B', 'B', 'B'), ('B', 'B', 'C'), ('B', 'C', 'A'), ('B', 'C', 'B'), ('B', 'C', 'C'), ('C', 'A', 'A'), ('C', 'A', 'B'), ('C', 'A', 'C'), ('C', 'B', 'A'), ('C', 'B', 'B'), ('C', 'B', 'C'), ('C', 'C', 'A'), ('C', 'C', 'B'), ('C', 'C', 'C')]\n"
     ]
    }
   ],
   "source": [
    "unique_tags = set()\n",
    "for sentence in training:\n",
    "    tags = [pair[1] for pair in sentence]\n",
    "    unique_tags = unique_tags | set(tags)\n",
    "\n",
    "combo = product(['A','B','C'], repeat=3)\n",
    "print list(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11354)\n"
     ]
    }
   ],
   "source": [
    "phi = Phi()\n",
    "\n",
    "phi.fit(training)\n",
    "\n",
    "sentence = testing[0]\n",
    "words = [pair[0] for pair in sentence]\n",
    "tags = [pair[1] for pair in sentence]\n",
    "\n",
    "vector = phi.transform(words, tags)\n",
    "print vector.shape\n",
    "#count = phi.inverse_transform(vector)\n",
    "#print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    }
   ],
   "source": [
    "reduced_training = []\n",
    "for sentence in training:\n",
    "    words = [pair[0] for pair in sentence]\n",
    "    if len(words) < 8:\n",
    "        reduced_training.append(sentence)\n",
    "        \n",
    "print len(reduced_training)\n",
    "\n",
    "training_examples = []\n",
    "for sentence in reduced_training:\n",
    "    words = [pair[0] for pair in sentence]\n",
    "    tags = [pair[1] for pair in sentence]\n",
    "    training_examples.append((words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098\n"
     ]
    }
   ],
   "source": [
    "reduced_testing = []\n",
    "for sentence in testing:\n",
    "    words = [pair[0] for pair in sentence]\n",
    "    if len(words) < 8:\n",
    "        reduced_test.append(sentence)\n",
    "\n",
    "print len(reduced_test)\n",
    "\n",
    "testing_examples = []\n",
    "for sentence in reduced_training:\n",
    "    words = [pair[0] for pair in sentence]\n",
    "    tags = [pair[1] for pair in sentence]\n",
    "    testing_examples.append((words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ADJ', u'ADV', u'ADV', u'ADV', u'ADV', u'ADV', u'ADV']\n"
     ]
    }
   ],
   "source": [
    "words = training_examples[11][0]\n",
    "prediction = viterbi(words, weights, phi, unique_tags)\n",
    "print prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-425-9ef5d53a50c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructured_perceptron_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-420-26130221abfe>\u001b[0m in \u001b[0;36mstructured_perceptron_train\u001b[1;34m(examples, weights, phi, unique_tags, iterations, average_weights, shuffle)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mtag_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mpredicted_tag_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructured_perceptron_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpredicted_tag_sequence\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtag_sequence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-419-73170d5deb33>\u001b[0m in \u001b[0;36mstructured_perceptron_predict\u001b[1;34m(word_sequence, weights, phi, unique_tags)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#return list(tag_sequences[np.argmax(scores)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#print word_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-399-fd486ae38f60>\u001b[0m in \u001b[0;36mviterbi\u001b[1;34m(word_sequence, weights, phi, unique_tags)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnext_tag_index\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrellis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mcurrent_item\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_tag_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mpredicted_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_item\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mnext_tag_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_item\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "weights = csr_matrix((1,vector.shape[1]))\n",
    "new_weights = structured_perceptron_train(training_examples, weights, phi, unique_tags, 1, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 s, sys: 47 ms, total: 1.93 s\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "words = [example[0] for example in testing_examples]\n",
    "%time predictions = structured_perceptron_test(words, new_weights, phi, unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Accuracy:  0.528735632184\n",
      "Word Accuracy:  0.614379084967\n"
     ]
    }
   ],
   "source": [
    "correct_word = 0\n",
    "total_word = 0\n",
    "correct_sentence = 0\n",
    "for i in range(0,len(predictions)):\n",
    "    real = testing_examples[i][1]\n",
    "    predicted = predictions[i]\n",
    "    \n",
    "    if real == predicted:\n",
    "        correct_sentence += 1\n",
    "    \n",
    "    for j in range(0,len(real)):\n",
    "        total_word += 1\n",
    "        if real[j] == predicted[j]:\n",
    "            correct_word += 1\n",
    "\n",
    "print 'Sentence Accuracy: ', correct_sentence / float(len(predictions))\n",
    "print 'Word Accuracy: ', correct_word / float(total_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def structured_perceptron_predict(word_sequence, weights, phi, unique_tags):\n",
    "    length = len(word_sequence)\n",
    "    \n",
    "    #Converts the iterable into a list in order to return the best one\n",
    "    #Might cause overflow error for long word sequences >10\n",
    "    #tag_sequences = list(product(unique_tags, repeat=length))\n",
    "    \n",
    "    #scores = []\n",
    "    #for tag_sequence in tag_sequences:\n",
    "    #    tag_sequence = list(tag_sequence)\n",
    "    #    vector = phi.transform(word_sequence, tag_sequence).toarray()\n",
    "    #    score = np.dot(weights.toarray(), vector.T)\n",
    "    #    scores.append(score)\n",
    "\n",
    "    #return list(tag_sequences[np.argmax(scores)])\n",
    "    #print word_sequence\n",
    "    return viterbi(word_sequence, weights, phi, unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def structured_perceptron_train(examples, weights, phi, unique_tags, iterations=1, average_weights=False, shuffle=True):\n",
    "    for i in range(0,iterations):\n",
    "        if shuffle:\n",
    "            np.random.shuffle(examples)\n",
    "\n",
    "        #if AVERAGE_WEIGHTS:\n",
    "        #    weights_avg = csr_matrix((1,examples[0].shape[1]))\n",
    "\n",
    "        count = 0\n",
    "        for example in examples:\n",
    "            #print count\n",
    "            word_sequence = example[0]\n",
    "            tag_sequence = example[1]\n",
    "            \n",
    "            predicted_tag_sequence = structured_perceptron_predict(word_sequence, weights, phi, unique_tags)\n",
    "\n",
    "            if predicted_tag_sequence != tag_sequence:\n",
    "                weights += phi.transform(word_sequence, tag_sequence) - phi.transform(word_sequence, predicted_tag_sequence)\n",
    "            \n",
    "            #count += 1\n",
    "            \n",
    "            #if bool(prediction) != label:\n",
    "            #    if prediction == 1:\n",
    "            #        weights[0] += vector\n",
    "            #        weights[1] -= vector\n",
    "            #    elif prediction == 0:\n",
    "            #        weights[1] += vector\n",
    "            #        weights[0] -= vector\n",
    "\n",
    "            #    if AVERAGE_WEIGHTS:\n",
    "            #        count += 1\n",
    "            #        # Rolling average using\n",
    "            #        # avg -= avg / count\n",
    "            #        # avg += new_sample / count\n",
    "            #        weights_avg -= weights_avg / count\n",
    "            #        weights_avg += weights / count\n",
    "\n",
    "    #if AVERAGE_WEIGHTS:\n",
    "    #    #average = weights_sum / float(iterations * len(examples))\n",
    "    #    return weights_avg\n",
    "    #else:\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def structured_perceptron_test(word_sequences, weights, phi, unique_tags):\n",
    "    predictions = []\n",
    "    for word_sequence in word_sequences:\n",
    "        \n",
    "        prediction = structured_perceptron_predict(word_sequence, weights, phi, unique_tags)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(word_sequence, weights, phi, unique_tags):\n",
    "    #print 'Viterbi'\n",
    "\n",
    "    trellis = [{}]\n",
    "    \n",
    "    #Set the start scores in the trellis\n",
    "    for tag in tags:\n",
    "        trellis[0][tag] = (0, -1)\n",
    "\n",
    "    #Iterate through the trellis updating the scores and back pointers as you go\n",
    "    for t in range(1,len(word_sequence)):\n",
    "        trellis.append({})\n",
    "        for current_tag in unique_tags:\n",
    "            scores = []\n",
    "            for previous_tag in tags:\n",
    "                vector = phi.transform([word_sequence[t-1], word_sequence[t]], [previous_tag, current_tag])\n",
    "                score = np.dot(weights.toarray(), vector.toarray().T) + trellis[t-1][previous_tag][0]\n",
    "                scores.append(score[0][0])\n",
    "\n",
    "            max_score = max(scores)\n",
    "            back_pointer = np.argmax(scores)\n",
    "            trellis[t][current_tag] = (max_score, back_pointer)\n",
    "    \n",
    "    #Calculate the tag index of the higest scoring tag in the final row of the trellis\n",
    "    index = len(trellis)-1\n",
    "    predicted_tags = []\n",
    "    items = trellis[index].items()\n",
    "    scores = [pair[1][0] for pair in items]\n",
    "    next_tag_index = np.argmax(scores)\n",
    "\n",
    "    #Follow the back pointers until you reach the end of the trellis or reach the start tag of -1\n",
    "    while index >= 0 and next_tag_index >= 0:\n",
    "        items = trellis[index].items()\n",
    "        current_item = items[next_tag_index]\n",
    "        predicted_tags.append(current_item[0])\n",
    "        next_tag_index = current_item[1][1]\n",
    "        index -= 1\n",
    "\n",
    "    predicted_tags.reverse()\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'PRON', u'VERB']\n"
     ]
    }
   ],
   "source": [
    "words = testing_examples[72][0]\n",
    "tags = list(unique_tags)\n",
    "\n",
    "print viterbi(words, new_weights, phi, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
